{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers accelerate safetensors"
      ],
      "metadata": {
        "id": "ibADKIOpvp-T"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command `!pip install -qU transformers accelerate safetensors` installs or updates three Python libraries.\n",
        "\n",
        "* `!pip install`: This is a standard command for installing Python packages.\n",
        "* `-q`: This option means \"quiet,\" so less output is shown during installation.\n",
        "* `-U`: This option means \"upgrade,\" so if the packages are already installed, they will be updated to the newest version.\n",
        "\n",
        "The packages being installed are:\n",
        "\n",
        "* **transformers**: A library from Hugging Face that provides pre-trained models for Natural Language Processing (NLP) tasks like text summarization, translation, and question-answering.\n",
        "* **accelerate**: A library that helps run PyTorch code on different kinds of hardware (like GPUs or TPUs) with minimal code changes.\n",
        "* **safetensors**: A file format for storing large and complex data structures (tensors) safely and efficiently."
      ],
      "metadata": {
        "id": "IbYNN8wXyrYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daGwWJ5Qu5g0",
        "outputId": "20fc9484-4ade-4eea-ced0-79b5095b0a41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/diffusers.git\n",
            "  Cloning https://github.com/huggingface/diffusers.git to /tmp/pip-req-build-xu6qefad\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers.git /tmp/pip-req-build-xu6qefad\n",
            "  Resolved https://github.com/huggingface/diffusers.git to commit 425a715e35479338c06b2a68eb3a95790c1db3c5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (0.33.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (11.2.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers==0.35.0.dev0) (3.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.35.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.35.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.35.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.35.0.dev0) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command installs the `diffusers` library directly from its source code on GitHub.\n",
        "\n",
        "Instead of getting a stable version from the Python Package Index (PyPI), this command uses **Git**, a version control system, to download the most current code from the Hugging Face `diffusers` repository. This is often done to get the very latest features or bug fixes that have not yet been released in an official package version. In this instance we need to be able to use the FluxKontextPipeline."
      ],
      "metadata": {
        "id": "StyFZpjczEqj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XO_LoJRvRMnN"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "from diffusers import FluxKontextPipeline\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports several Python libraries needed to run an AI image generation model and sets an environment variable for faster model downloads.\n",
        "\n",
        "---\n",
        "## Library Imports\n",
        "\n",
        "* **`import gradio as gr`**: Imports the **Gradio** library, which is used to create simple web interfaces for machine learning models.\n",
        "* **`import numpy as np`**: Imports the **NumPy** library, a fundamental package for scientific computing and working with arrays of numbers.\n",
        "* **`import os`**: Imports the **os** module, which allows the program to interact with the operating system, for instance, to manage files and directories.\n",
        "* **`import torch`**: Imports **PyTorch**, a popular machine learning framework that provides tools for building and training neural networks.\n",
        "* **`import random`**: Imports the **random** module for generating random numbers.\n",
        "* **`from PIL import Image`**: Imports the **Image** module from the Python Imaging Library (PIL), used for opening, manipulating, and saving many different image file formats.\n",
        "* **`from diffusers import FluxKontextPipeline`**: From the **`diffusers`** library, this imports a specific image generation model pipeline named **`FluxKontextPipeline`**.\n",
        "* **`from diffusers.utils import load_image`**: Imports a helper function named **`load_image`** from the `diffusers` library to easily load images.\n",
        "\n",
        "---\n",
        "## Environment Variable\n",
        "\n",
        "* **`os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"`**: This line sets an environment variable that enables a faster library called `hf_transfer` for downloading models and datasets from the Hugging Face Hub. This speeds up the initial setup process."
      ],
      "metadata": {
        "id": "nNJvoQ3ozVVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    model = \"black-forest-labs/FLUX.1-Kontext-dev\"\n",
        "    device = 'cuda'\n",
        "    dtype = torch.bfloat16\n",
        "    variant = \"fp16\"\n",
        "    seed = 42"
      ],
      "metadata": {
        "id": "r6vkE2tWwd6m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a configuration class named `CFG`.\n",
        "\n",
        "The class acts as a container to hold various settings for a machine learning script. This approach groups all the important parameters in one place, making them easy to find and change.\n",
        "\n",
        "---\n",
        "### **Configuration Parameters**\n",
        "\n",
        "* **`model`**: This sets the model identifier to **`\"black-forest-labs/FLUX.1-Kontext-dev\"`**. This string is the name of a pre-trained model, hosted on the Hugging Face Hub.\n",
        "* **`device`**: This specifies the hardware device for running the model as **`'cuda'`**, which refers to a NVIDIA GPU. Using a GPU significantly speeds up calculations for machine learning models.\n",
        "* **`dtype`**: This sets the data type for the model's calculations to **`torch.bfloat16`**. This is a 16-bit floating-point format from the PyTorch library that can speed up computations and reduce memory use while maintaining a good level of precision.\n",
        "* **`variant`**: This sets the model variant to **`\"fp16\"`**. This indicates that a version of the model using 16-bit floating-point precision should be used.\n",
        "* **`seed`**: This sets a random seed to **`42`**. Setting a seed ensures that any process involving randomness (like initializing model weights) will produce the exact same results every time the code is run."
      ],
      "metadata": {
        "id": "0vf1AaKW27v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = FluxKontextPipeline.from_pretrained(CFG.model, torch_dtype= CFG.dtype).to(CFG.device)\n"
      ],
      "metadata": {
        "id": "KTyIEaddrvx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code initializes and prepares an AI model for use.\n",
        "\n",
        "It takes the **`FluxKontextPipeline`** model and performs two main actions:\n",
        "\n",
        "1.  **`from_pretrained(CFG.model, torch_dtype=CFG.dtype)`**: This part loads the pre-trained model identified by `CFG.model`. It also sets the model's numerical precision to `CFG.dtype` (`torch.bfloat16`) to optimize its performance and memory usage.\n",
        "\n",
        "2.  **`.to(CFG.device)`**: After loading, this method moves the entire model onto the specified hardware, which is the **GPU** (`'cuda'`). Running the model on a GPU is much faster than using a CPU.\n",
        "\n",
        "The final, ready-to-use model is then stored in the variable named `pipe`."
      ],
      "metadata": {
        "id": "9_sNetz03d9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(input_image, prompt,  guidance_scale=2.5, steps=28, progress=gr.Progress(track_tqdm=True)):\n",
        "\n",
        "    input_image = input_image.convert(\"RGB\")\n",
        "    image = pipe(\n",
        "        image=input_image,\n",
        "        prompt=prompt,\n",
        "        guidance_scale=guidance_scale,\n",
        "        width = input_image.size[0],\n",
        "        height = input_image.size[1],\n",
        "        num_inference_steps=steps,\n",
        "        generator=torch.Generator().manual_seed(CFG.seed),\n",
        "    ).images[0]\n",
        "\n",
        "    return image, gr.Button(visible=True)"
      ],
      "metadata": {
        "id": "xAfwR2yinjyW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python code defines a function named `infer` that generates a new image based on an existing image and a text description.\n",
        "\n",
        "### Function Purpose\n",
        "\n",
        "The **`infer`** function is designed to be the core logic for an image editing or generation task. It takes an input image, a text **`prompt`**, and a few settings to create an output image. The `progress` parameter shows it's intended to be used with a Gradio interface to display a progress bar.\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-Step Process\n",
        "\n",
        "1.  **Convert Image Format**: The line `input_image = input_image.convert(\"RGB\")` ensures the input image is in the standard **RGB** (Red, Green, Blue) color format, which is a common requirement for image models.\n",
        "\n",
        "2.  **Generate New Image**: The function then calls the **`pipe`** object (the model pipeline loaded earlier). It provides the model with all the necessary information:\n",
        "    * **`image`**: The original input image.\n",
        "    * **`prompt`**: The text instructions for how to change the image.\n",
        "    * **`guidance_scale`**: A number that controls how strictly the model should follow the prompt.\n",
        "    * **`width`** and **`height`**: These are set to the dimensions of the input image, so the output image has the same size.\n",
        "    * **`num_inference_steps`**: The number of steps the model takes to generate the image.\n",
        "    * **`generator`**: This creates a random number generator with a fixed **seed** (`CFG.seed`). Using a seed ensures that the generation process is repeatable and will produce the same output for the same inputs.\n",
        "\n",
        "3.  **Return Output**: The function returns two items:\n",
        "    * The newly generated **`image`**.\n",
        "    * A Gradio button object that is set to be visible."
      ],
      "metadata": {
        "id": "CAfsA3Mw4C4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "\n",
        "    with gr.Column(elem_id=\"col-container\"):\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                input_image = gr.Image(label=\"Upload the image for editing\", type=\"pil\")\n",
        "                with gr.Row():\n",
        "                    prompt = gr.Text(\n",
        "                        label=\"Prompt\",\n",
        "                        show_label=False, max_lines=1,\n",
        "                        placeholder=\"Enter your prompt for editing\",\n",
        "                        container=False,\n",
        "                    )\n",
        "                    run_button = gr.Button(\"Run\", scale=0)\n",
        "\n",
        "                with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "\n",
        "\n",
        "                    guidance_scale = gr.Slider(\n",
        "                        label=\"Guidance Scale\", minimum=1,  maximum=10, step=0.1, value=2.5, )\n",
        "\n",
        "                    steps = gr.Slider( label=\"Steps\",  minimum=1, maximum=30,  value=28, step=1  )\n",
        "\n",
        "            with gr.Column():\n",
        "                result = gr.Image(label=\"Result\", show_label=False, interactive=False)\n",
        "                reuse_button = gr.Button(\"Reuse this image\", visible=False)\n",
        "\n",
        "\n",
        "    gr.on(\n",
        "        triggers=[run_button.click, prompt.submit],\n",
        "        fn = infer,\n",
        "        inputs = [input_image, prompt, guidance_scale, steps],\n",
        "        outputs = [result, reuse_button]\n",
        "    )\n",
        "    reuse_button.click(\n",
        "        fn = lambda image: image,\n",
        "        inputs = [result],\n",
        "        outputs = [input_image]\n",
        "    )"
      ],
      "metadata": {
        "id": "dTkvlRPAnx-7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the **Gradio** library to create a web interface for the image generation function described previously.\n",
        "\n",
        "The interface allows a user to upload an image, type a text prompt, adjust settings, and see the resulting edited image.\n",
        "\n",
        "---\n",
        "\n",
        "### UI Layout\n",
        "\n",
        "The code defines the visual layout of the web application.\n",
        "\n",
        "* **Main Container**: The entire interface is organized into a main column.\n",
        "* **Two-Column Layout**: Inside, the layout is split into two columns, side-by-side.\n",
        "    * **Left Column (Inputs)**:\n",
        "        * An **`Image`** upload box for the user to provide the initial picture.\n",
        "        * A **`Text`** input box for the user to type their prompt (e.g., \"make it look like a watercolor painting\").\n",
        "        * A **`Button`** labeled \"**Run**\" to start the process.\n",
        "        * An **`Accordion`** labeled \"**Advanced Settings**\" which is closed by default. It contains two **`Slider`** controls:\n",
        "            * **Guidance Scale**: To adjust how strongly the model follows the prompt.\n",
        "            * **Steps**: To control the number of generation steps.\n",
        "    * **Right Column (Outputs)**:\n",
        "        * An **`Image`** display area to show the final **`Result`**.\n",
        "        * A hidden **`Button`** labeled \"**Reuse this image**.\"\n",
        "\n",
        "---\n",
        "\n",
        "### Functionality\n",
        "\n",
        "This part of the code connects the user interface elements to Python functions.\n",
        "\n",
        "* **Generating an Image**:\n",
        "    * **Trigger**: Clicking the \"**Run**\" button or pressing Enter in the prompt box.\n",
        "    * **Action**: This calls the **`infer`** function.\n",
        "    * **Inputs**: It sends the uploaded image, the prompt text, and the values from the two sliders (`guidance_scale` and `steps`) to the `infer` function.\n",
        "    * **Outputs**: The generated image is displayed in the **`result`** box, and the \"**Reuse this image**\" button becomes visible.\n",
        "\n",
        "* **Reusing an Image**:\n",
        "    * **Trigger**: Clicking the \"**Reuse this image**\" button.\n",
        "    * **Action**: It takes the image from the **`result`** box and moves it to the **`input_image`** box on the left. This allows the user to perform further edits on the newly created image."
      ],
      "metadata": {
        "id": "cMwKUhae4bXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "B8a336sdn2-a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}